<!DOCTYPE html>
<html>
<head>
    

    

    
<!-- Baidu Tongji -->
<script>var _hmt = _hmt || []</script>
<script async src="//hm.baidu.com/hm.js?29d5d4407fd60d03b6916e1bfd41d668"></script>
<!-- End Baidu Tongji -->




    <meta charset="utf-8">
    
    
    
    
    <title>scrapy爬虫总体笔记记录 | afacode的博客 | 人懒，偶尔记录</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="mysql,python,scrapy,mongodb">
    <meta name="description" content="scrapy爬虫 - 记忆笔记 - 只供自己查找">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy爬虫总体笔记记录">
<meta property="og:url" content="https://blog.afacode.top/2019/05/27/python-scrapy-text/index.html">
<meta property="og:site_name" content="afacode的博客">
<meta property="og:description" content="scrapy爬虫 - 记忆笔记 - 只供自己查找">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-05-27T15:53:25.000Z">
<meta property="article:modified_time" content="2023-07-13T14:40:42.873Z">
<meta property="article:author" content="afacode(阿发)">
<meta property="article:tag" content="mysql">
<meta property="article:tag" content="python">
<meta property="article:tag" content="scrapy">
<meta property="article:tag" content="mongodb">
<meta name="twitter:card" content="summary">
    
        <link rel="alternate" type="application/atom+xml" title="afacode的博客" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">afacode(阿发)</h5>
          <a href="mailto:afacode@outlook.com" title="afacode@outlook.com" class="mail">afacode@outlook.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                index
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/afacode" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                about
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">scrapy爬虫总体笔记记录</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">scrapy爬虫总体笔记记录</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-05-27T15:53:25.000Z" itemprop="datePublished" class="page-time">
  2019-05-27
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/python/">python</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#安装"><span class="post-toc-number">1.</span> <span class="post-toc-text">安装</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#使用"><span class="post-toc-number">2.</span> <span class="post-toc-text">使用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#创建项目"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">创建项目</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#创建爬虫"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">创建爬虫</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#运行"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">运行</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#选择器-提取数据"><span class="post-toc-number">3.</span> <span class="post-toc-text">选择器(提取数据)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#css"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">css</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#regx"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">regx</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Splash"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Splash</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#xpath"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">xpath</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#spider-函数解析"><span class="post-toc-number">4.</span> <span class="post-toc-text">spider 函数解析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#请求-打开页面"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">请求(打开页面)</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#抓取数据"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">抓取数据</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#中间件-pipeline"><span class="post-toc-number">5.</span> <span class="post-toc-text">中间件(pipeline)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#图片下载中间件"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">图片下载中间件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#图片重命名、放入不同文件夹"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">图片重命名、放入不同文件夹</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#图片防盗链下载-middlewares-py"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">图片防盗链下载 middlewares.py</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#mysql"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">mysql</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#mongodb"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">mongodb</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#部署-scrapyd"><span class="post-toc-number">6.</span> <span class="post-toc-text">部署 scrapyd</span></a></li></ol>
        </nav>
    </aside>


<article id="post-python-scrapy-text"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">scrapy爬虫总体笔记记录</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-05-27 23:53:25" datetime="2019-05-27T15:53:25.000Z"  itemprop="datePublished">2019-05-27</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/python/">python</a></li></ul>



            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>scrapy爬虫 - 记忆笔记 - 只供自己查找</p>
<a id="more"></a>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>pip3 install scrapy</code></p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p><code>scrapy startproject example</code></p>
<h3 id="创建爬虫"><a href="#创建爬虫" class="headerlink" title="创建爬虫"></a>创建爬虫</h3><p><code>scrapy genspider book_spider</code></p>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p><code> scrapy crawl book_spider -o books.csv</code></p>
<h2 id="选择器-提取数据"><a href="#选择器-提取数据" class="headerlink" title="选择器(提取数据)"></a>选择器(提取数据)</h2><h3 id="css"><a href="#css" class="headerlink" title="css"></a>css</h3><p><a href="http://www.scrapyd.cn/doc/146.html" target="_blank" rel="noopener">css选择器</a></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选中所有的img</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'img'</span>)</span><br><span class="line"><span class="comment"># 选中div后代中的img（后代元素包括子代、孙代等）</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'div img'</span>)</span><br><span class="line"><span class="comment"># 选中body子元素中的div元素</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'body&gt;div'</span>)</span><br><span class="line"><span class="comment"># 选中包含style属性的元素</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'[style]'</span>)</span><br><span class="line"><span class="comment"># 选中属性id值为images-1的元素</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'[id=images-1]'</span>)</span><br><span class="line"><span class="comment"># 选中CLASS属性包含img的a元素</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'a.img'</span>)</span><br><span class="line"><span class="comment"># 选中CLASS属性包含img的元素</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'.img'</span>)</span><br><span class="line"><span class="comment"># 选中所有a的文本</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'a::text'</span>).extract()</span><br><span class="line"><span class="comment"># 选中所有a的href属性文本 extract_first()</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.css(<span class="string">'a::attr(href)'</span>).extract()</span><br></pre></td></tr></table></figure>
<h3 id="regx"><a href="#regx" class="headerlink" title="regx"></a>regx</h3><h3 id="Splash"><a href="#Splash" class="headerlink" title="Splash"></a>Splash</h3><p>Splash 是 Scrapy 官方推荐的 JavaScript 渲染引擎，是一款轻量级的无界面浏览器，类似于 PhantomJS</p>
<blockquote>
<p>pip3 install scrapy-splash</p>
</blockquote>
<ul>
<li>SplashRequest,用户只需使用scrapy_splash.SplashRequest（替代scrapy.Request）提交请求即可<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from scrapy_splash import SplashRequest</span><br><span class="line"><span class="function"><span class="title">SplashRequest</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure></li>
<li>下面是 SplashRequest 构造器方法中的一些常用参数。 <ul>
<li>① url ：与scrapy.Request中的url相同，也就是待爬取页面的url（注意，不是Splash服务器地址）。</li>
<li>② headers: 与scrapy.Request中的headers相同。</li>
<li>③ cookies：与scrapy.Request中的cookies相同。</li>
<li>④ args：传递给Splash的参数（除url以外），如wait、timeout、images、js_source等。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>创建项目</li>
<li>setting.py<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 1. Splash服务器地址</span></span><br><span class="line">SPLASH_URL = <span class="string">'http://localhost:8050'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 2. 开启Splash的两个下载中间件并调整HttpCompressionMiddleware的次序</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line"><span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line"><span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># 3. 设置去重过滤器</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 4. 用来支持cache_args（可选）</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line"><span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>修改Spider<br><code># 把 ScrapyRequest(url) 改为SplashRequest(url, args={&#39;images&#39;:0,&#39;timeout&#39;=3})</code></li>
</ul>
<h3 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h3><p><a href="http://www.scrapyd.cn/doc/186.html" target="_blank" rel="noopener">xpath</a></p>
<h2 id="spider-函数解析"><a href="#spider-函数解析" class="headerlink" title="spider 函数解析"></a>spider 函数解析</h2><h3 id="请求-打开页面"><a href="#请求-打开页面" class="headerlink" title="请求(打开页面)"></a>请求(打开页面)</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">simpleUrl</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">"simpleUrl"</span></span><br><span class="line">    start_urls = [<span class="string">'http://books.toscrape.com/'</span>]</span><br><span class="line">    或者</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        urls = [ <span class="comment">#爬取的链接由此方法通过下面链接爬取页面</span></span><br><span class="line">    		 <span class="string">'http://lab.scrapyd.cn/page/1/'</span>,</span><br><span class="line">    		 <span class="string">'http://lab.scrapyd.cn/page/2/'</span>,</span><br><span class="line">    	]</span><br><span class="line">    	<span class="keyword">for</span> url <span class="keyword">in</span> <span class="symbol">urls:</span></span><br><span class="line">    		<span class="comment">#发送请求</span></span><br><span class="line">    		 <span class="keyword">yield</span> scrapy.Request(url=url, callback=<span class="keyword">self</span>.parse)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="抓取数据"><a href="#抓取数据" class="headerlink" title="抓取数据"></a>抓取数据</h3><ul>
<li>有些数据信息需要提取两个页面才能完成，那么如何把上一个def中的数据，传入下一个def中呢<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造字典 meta=&#123;'_book_item':book_item&#125;，通过scrapy.Request传入</span></span><br><span class="line"><span class="comment"># 下一个def 通过book_item = response.meta['_book_item']获得数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url, headers= &#123;<span class="string">'User-Agent'</span><span class="symbol">:<span class="string">'Mozilla/5.0'</span></span>&#125;, callback=<span class="keyword">self</span>.book_parse, meta=&#123;<span class="string">'_book_item'</span><span class="symbol">:book_item</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_book</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    book_item = response.meta[<span class="string">'_book_item'</span>]</span><br></pre></td></tr></table></figure></li>
<li>scrapy 爬网站 显示 Filtered offsite request to 错误</li>
</ul>
<p>官方对这个的解释，是你要request的地址和allow_domain里面的冲突，从而被过滤掉。可以停用过滤功能<br><code>yield Request(url, callback=self.parse_item, dont_filter=True)</code></p>
<h2 id="中间件-pipeline"><a href="#中间件-pipeline" class="headerlink" title="中间件(pipeline)"></a>中间件(pipeline)</h2><ol>
<li>Item Pipeline 可以处理、存储数据</li>
<li>处理从 spider 发送的，由Item 封装的数据</li>
<li>页面数据抓取用spider解析，其他用Item Pipeline</li>
<li>主要的三个模块： <ul>
<li>① process_item(item, spider)：处理数据 </li>
<li>② open_spider(self, spider)：打开文件或数据库 </li>
<li>③ close_spider(self, spider)：关闭文件或数据库</li>
<li></li>
</ul>
</li>
<li>处理数据步骤： <ul>
<li>① 打开pipelines.py，修改文件 </li>
<li>② 打开setting.py，修改配置文件</li>
</ul>
</li>
</ol>
<h3 id="图片下载中间件"><a href="#图片下载中间件" class="headerlink" title="图片下载中间件"></a>图片下载中间件</h3><p>打开<code>pipeline.py</code>进行中间件编写，这里的话主要继承了scrapy的：<code>ImagesPipeline</code>这个类，我们需要在里面实现：<code>def get_media_requests(self, item, info)</code>这个方法，这个方法主要是把蜘蛛yield过来的图片链接执行下载</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="symbol">ImagespiderPipeline</span>(<span class="symbol">ImagesPipeline</span>):</span><br><span class="line"></span><br><span class="line">    <span class="symbol">def</span> <span class="symbol">get_media_requests</span>(<span class="symbol">self, <span class="symbol">item</span>, <span class="symbol">info</span></span>):</span><br><span class="line">        # 循环每一张图片地址下载，若传过来的不是集合则无需循环直接<span class="symbol">yield</span></span><br><span class="line">        <span class="symbol">for</span> <span class="symbol">image_url</span> <span class="symbol">in</span> <span class="symbol">item</span>['<span class="symbol">imgurl</span>']:</span><br><span class="line">            <span class="symbol">yield</span> <span class="symbol">Request</span>(<span class="symbol">image_url</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>设置，启动图片下载 <code>settings</code></p>
</blockquote>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#图片存储位置</span></span><br><span class="line">IMAGES_STORE = <span class="string">'D:\ImageSpider'</span></span><br><span class="line"><span class="meta">#启动图片下载中间件</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'ImageSpider.pipelines.ImagespiderPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="图片重命名、放入不同文件夹"><a href="#图片重命名、放入不同文件夹" class="headerlink" title="图片重命名、放入不同文件夹"></a>图片重命名、放入不同文件夹</h3><p><code>ImagesPipeline</code>的一些实现，你会发现里面有这么一个方法：<code>def file_path(self, request, response=None, info=None)</code> 这个方法便是图片重命名以及目录归类的方法，我们只需要重写里面的一些内容，便可轻松实现scrapy图片重命名，图片保存不同目录</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagespiderPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 重命名，若不重写这函数，图片名为哈希，就是一串乱七八糟的名字</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取url前面名称作为图片名。</span></span><br><span class="line">        image_guid = request.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 接收上面meta传递过来的图片名称</span></span><br><span class="line">        name = request.meta[<span class="string">'name'</span>]</span><br><span class="line">        <span class="comment"># 过滤windows字符串，不经过这么一个步骤，你会发现有乱码或无法下载</span></span><br><span class="line">        name = re.sub(<span class="string">r'[？\\*|“&lt;&gt;:/]'</span>, <span class="string">''</span>, name)</span><br><span class="line">        <span class="comment"># 分文件夹存储的关键：&#123;0&#125;对应着name；&#123;1&#125;对应着image_guid</span></span><br><span class="line">        filename = <span class="string">u'&#123;0&#125;/&#123;1&#125;'</span>.format(name, image_guid)</span><br><span class="line">        <span class="keyword">return</span> filename</span><br></pre></td></tr></table></figure>
<h3 id="图片防盗链下载-middlewares-py"><a href="#图片防盗链下载-middlewares-py" class="headerlink" title="图片防盗链下载 middlewares.py"></a>图片防盗链下载 <code>middlewares.py</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AoisolasSpiderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">'''设置headers和切换请求头'''</span></span><br><span class="line">        referer = request.url</span><br><span class="line">        <span class="keyword">if</span> referer:</span><br><span class="line">            request.headers[<span class="string">'referer'</span>] = referer</span><br></pre></td></tr></table></figure>

<h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><p> 版本 <code>&gt;3.4</code> PyMySQL，  <code>2 - &lt;3.4</code> MySQLdb </p>
<blockquote>
<p>pip3 install pymysql</p>
</blockquote>
<ul>
<li>必须实现 <code>process_item(self, item, spider)</code> 方法</li>
</ul>
<p>这个方法有两个参数，一个是item，一个是spider。item就是爬取到的数据，执行完数据库插入之后，需要执行返回，也就是需要：return item，无论你是插入mysql、mongodb还是其他数据库，都必须实现这么一个方法</p>
<ul>
<li><code>open_spider(self, spider)</code> 爬虫打开时执行</li>
<li><code>close_spider(self, spider)</code> 爬虫关闭时执行<blockquote>
<p>根据自己的需要，按需实现，并非必须方法！</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>新建 MySQLPipline.py mysql中间件</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql.cursors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQLPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 连接数据库</span></span><br><span class="line">        self.connect = pymysql.connect(</span><br><span class="line">            host=<span class="string">'127.0.0.1'</span>,  <span class="comment"># 数据库地址</span></span><br><span class="line">            port=<span class="number">3306</span>,  <span class="comment"># 数据库端口</span></span><br><span class="line">            db=<span class="string">'scrapyMysql'</span>,  <span class="comment"># 数据库名</span></span><br><span class="line">            user=<span class="string">'root'</span>,  <span class="comment"># 数据库用户名</span></span><br><span class="line">            passwd=<span class="string">'root'</span>,  <span class="comment"># 数据库密码</span></span><br><span class="line">            charset=<span class="string">'utf8'</span>,  <span class="comment"># 编码方式</span></span><br><span class="line">            use_unicode=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 通过cursor执行增删查改</span></span><br><span class="line">        self.cursor = self.connect.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.cursor.execute(</span><br><span class="line">            <span class="string">"""insert into chuanke(name, type, view, price ,teacher, url)</span></span><br><span class="line"><span class="string">            value (%s, %s, %s, %s, %s, %s)"""</span>,<span class="comment">#纯属python操作mysql知识，不熟悉请恶补</span></span><br><span class="line">            (item[<span class="string">'name'</span>],<span class="comment"># item里面定义的字段和表字段对应</span></span><br><span class="line">             item[<span class="string">'type'</span>],</span><br><span class="line">             item[<span class="string">'view'</span>],</span><br><span class="line">             item[<span class="string">'price'</span>],</span><br><span class="line">             item[<span class="string">'teacher'</span>],</span><br><span class="line">             item[<span class="string">'url'</span>]))</span><br><span class="line">             </span><br><span class="line">        <span class="comment"># 提交sql语句</span></span><br><span class="line">        self.connect.commit()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> item  <span class="comment"># 必须实现返回</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>settings.py启用MySQLPipline</p>
</blockquote>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'scrapyMysql.MySQLPipeline.MySQLPipeline'</span>: <span class="number">1</span>,</span><br><span class="line">     <span class="meta">#格式为：<span class="string">'项目名.文件名.类名'</span>：优先级（越小越大）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="mongodb"><a href="#mongodb" class="headerlink" title="mongodb"></a>mongodb</h3><blockquote>
<p>pip3 install pymongo</p>
</blockquote>
<ul>
<li>创建Pipeline,类似mysql。 InputmongodbPipeline</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputmongodbPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 建立MongoDB数据库连接</span></span><br><span class="line">        client = pymongo.MongoClient(<span class="string">'127.0.0.1'</span>, <span class="number">27017</span>)</span><br><span class="line">        <span class="comment"># 连接所需数据库,ScrapyChina为数据库名</span></span><br><span class="line">        db = client[<span class="string">'ScrapyChina'</span>]</span><br><span class="line">        <span class="comment"># 连接所用集合，也就是我们通常所说的表，mingyan为表名</span></span><br><span class="line">        <span class="keyword">self</span>.post = db[<span class="string">'mingyan'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        postItem = dict(item)  <span class="comment"># 把item转化成字典形式</span></span><br><span class="line">        <span class="keyword">self</span>.post.insert(postItem)  <span class="comment"># 向数据库插入一条记录</span></span><br><span class="line">        <span class="keyword">return</span> item  <span class="comment"># 会在控制台输出原item数据，可以选择不写</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>setting.py</p>
</blockquote>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ITEM_PIPELINES</span> = &#123;</span><br><span class="line">   <span class="string">'InputMongodb.inputMongodbPipeline.InputmongodbPipeline'</span>: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>scrapy存入mongodb比起存入mysql要简单得多，我们不用创建表，只需在程序里面创建相应库名、表名即可</p>
<h2 id="部署-scrapyd"><a href="#部署-scrapyd" class="headerlink" title="部署 scrapyd"></a>部署 scrapyd</h2><p>在每台Linux机子上安装好scrapyd，并开启scrapyd服务；然后我们在windows客户端，也就是开发爬虫的这台电脑，安装上scrapyd的客户端<code>scrapyd-client</code>，通过scrapyd-client把不同网站的爬虫发送到不同的服务器，然后我们只需在windows上就行<strong>修改、启动、停止</strong>爬虫操作，更自动化的是scrapyd给我提供了很python接口，我们可以通过python编程控制蜘蛛的运行</p>
<blockquote>
<p>pip3 install scrapyd-client 安装在开发机</p>
</blockquote>
<ul>
<li>scrapyd的话一般安装在Linux上面，负责开启Linux端口，供scrapyd-client调用<blockquote>
<p>pip3 install scrapyd </p>
</blockquote>
</li>
</ul>
    
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        

        
        如有问题可联系 Email:afacode@outlook.com 或 微信:afacode
        
    </div>
    
    <footer>
        <a href="https://blog.afacode.top">
            <img src="/img/avatar.jpg" alt="afacode(阿发)">
            afacode(阿发)
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mongodb/" rel="tag">mongodb</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/" rel="tag">mysql</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&title=《scrapy爬虫总体笔记记录》 — afacode的博客&pic=https://blog.afacode.top/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&title=《scrapy爬虫总体笔记记录》 — afacode的博客&source=scrapy爬虫 - 记忆笔记 - 只供自己查找" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.afacode.top/2019/05/27/python-scrapy-text/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《scrapy爬虫总体笔记记录》 — afacode的博客&url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&via=https://blog.afacode.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/06/05/linux-ssh-port/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">修改SSH端口与端口分配规则分类</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/05/25/python-selenium-webdriver-install/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Mac Python Selenium+WebDriver(chromedriver)安装下载</h4>
      </a>
    </div>
  
</nav>



    

















<section class="comments" id="comments">
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
        var id = location.pathname
        if (location.pathname.length > 50) {
          id = location.pathname.replace(/\/\d+\/\d+\/\d+\//, '').replace('/', '').substring(0, 50)
        }
        const gitalk = new Gitalk({
          clientID: '16e31c2c3358a34e2259',
          clientSecret: '4db152f6b7c930baf8d23d43e8634d93be9e6395',
          repo: 'afacode.github.io',
          owner: 'afacode',
          admin: ['afacode'],
          id: id,      // Ensure uniqueness and length less than 50
          title: document.title.split('|')[0],
          distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</section>




</article>



</div>

        <footer class="footer">
    <div class="top">
        

        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>afacode(阿发) &copy; 2017 - 2023</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">辽ICP备16018834号-2</a><br>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&title=《scrapy爬虫总体笔记记录》 — afacode的博客&pic=https://blog.afacode.top/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&title=《scrapy爬虫总体笔记记录》 — afacode的博客&source=scrapy爬虫 - 记忆笔记 - 只供自己查找" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.afacode.top/2019/05/27/python-scrapy-text/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《scrapy爬虫总体笔记记录》 — afacode的博客&url=https://blog.afacode.top/2019/05/27/python-scrapy-text/&via=https://blog.afacode.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://blog.afacode.top/2019/05/27/python-scrapy-text/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACJ0lEQVR42u3aUXbiMAwFUPa/aboABvqeHHrG8c1XDySOLx+qZOnxiK/ny/X529c7X596/fzd+pddGBgY2zLyTbSffN5oAk7ux8DAOIeRhMIE/+6eNmbme8PAwMD4zFhJDWfvxcDAwGiXm4XR/yjgYmBgbMVYLyzbNZPVvlKLY2BgbMjIg+bf//2V/gYGBsZWjGd5tQ2A2RbrXWFgYNyasbJ0OzDRpp51YYyBgXFrRtuYzGFtaG5bCHXMxsDA2JCRH2C1aWL7bD529o9vMTAwDmB8fqwoI4M0cTa0UZ8FYmBgHMCYJXMrxfDsUA8DA+McRl6Ofq8NkAf9YeWNgYFxa0aeJq4c5C21EzAwMA5gJAlfcqC2HtDzt/xy3IaBgXEjxqyU/R6pXWepIsfAwNiEsT5I0R6o1WE0Gb/AwMC4NWPWemwHJlZGLlowBgbG/Rjtgde1g19toH8Lw8DAOJKRJ39twZk3OIvWKQYGxq0Z6yNi+YYe8TUcDsPAwLg1I9/oVSVrEpTrsTMMDIwDGLMXzA7dZungL4EbAwPjYMasGTDbbpIyDgMuBgbGtoxneeVpX3u4P0sro7wVAwNjc8Z6sFuf8mgD/UrIxsDA2JcxO0q7eGCiXAEDA+NMRhIuZ+Voe/Sfh34MDAyMuolY4vOwW0+LYGBgHM+YHYrlYTf5aS77v4GBgbEVY6Ux2Q5hrIdXDAyM0xh56ZgPW7Rthvbtw6YmBgbGfowfzLf6CPhRCJMAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>








<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '你怎么走了！';
            clearTimeout(titleTime);
        } else {
            document.title = '欢迎讨论!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
